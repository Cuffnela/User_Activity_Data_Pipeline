{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Tracking User Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, you work at an ed-tech firm. You've created a service that\n",
    "delivers assessments, and now lots of different customers (e.g., Pearson) want\n",
    "to publish their assessments on it. You need to get ready for data scientists\n",
    "who work for these customers to run queries on the data. \n",
    "\n",
    "Included below are directions for data pipeline setup, assumptions about the data entered into the pipeline, table creation using pyspark and table writing to HDFS using parquet, as well as examples of applicable business questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Setup Linux Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker\n",
    "***\n",
    "##### Spin up and Check Cluster (run everytime to start up cluster)\n",
    "* remove stray containers or networks to avoid port conflicts\n",
    "\n",
    "```\n",
    "docker-compose up -d\n",
    "docker-compose ps\n",
    "docker ps -a\n",
    "```\n",
    "***\n",
    "### Kafka \n",
    "***\n",
    "##### Create kafka topic assessments\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --create --topic assessments --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "```\n",
    "##### Check kafka topic (optional)\n",
    "```\n",
    "docker-compose exec kafka kafka-topics --describe --topic assessments --zookeeper zookeeper:32181\n",
    "```\n",
    "\n",
    "##### Publish messages to kafka using kafkacat\n",
    "```\n",
    "docker-compose exec mids bash -c \"cat /w205/project-2-Cuffnela/assessment-attempts-20180128-121051-nested.json | jq '.[]' -c | kafkacat -P -b kafka:29092 -t assessments\"\n",
    "```\n",
    "***\n",
    "\n",
    "### PySpark \n",
    "***\n",
    "##### Spin up pyspark\n",
    "```\n",
    "docker-compose exec spark pyspark\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "There are some assumptions that we make about our data as we unroll and query the data. The assumptions we have made in this Jupyter notebook are the following:\n",
    "\n",
    "* User_id is unique to an exam taker. i.e. they have the same user_id for each exam they take within this system. And all entries have a user_id\n",
    "* The order of question_ids within the questions list is in the order the questions appear on the exam.\n",
    "* Exam names are consistent throughout the Json file. For example, the exam name is always Intermediate Python Programming and never abbreviated to Int. Python Programming or Python Programming (Intermediate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PySpark to Create Queriable Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subscribe to Kakfa assessments topic and cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_assessments = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"kafka:29092\").option(\"subscribe\",\"assessments\").option(\"startingOffsets\", \"earliest\").option(\"endingOffsets\", \"latest\").load() \n",
    "\n",
    "raw_assessments.cache()\n",
    "\n",
    "raw_assessments.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Assessments Table\n",
    "\n",
    "The assessments table unrolls the outer layer of the initial json file. Values within this table are still nested, which can be seen in the columns with map() values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assessments = raw_assessments.select(raw_assessments.value.cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import Row\n",
    "\n",
    "extracted_assessments = assessments.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.registerTempTable('assessments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|        base_exam_id|certification|           exam_name|   keen_created_at|             keen_id|    keen_timestamp|max_attempts|           sequences|          started_at|        user_exam_id|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717442.735266|5a6745820eb8ab000...| 1516717442.735266|         1.0|Map(questions -> ...|2018-01-23T14:23:...|6d4089e4-bde5-4a2...|\n",
      "|37f0a30a-7464-11e...|        false|Normal Forms and ...| 1516717377.639827|5a674541ab6b0a000...| 1516717377.639827|         1.0|Map(questions -> ...|2018-01-23T14:21:...|2fec1534-b41f-441...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...| 1516738973.653394|5a67999d3ed3e3000...| 1516738973.653394|         1.0|Map(questions -> ...|2018-01-23T20:22:...|8edbc8a8-4d26-429...|\n",
      "|4beeac16-bb83-4d5...|        false|The Principles of...|1516738921.1137421|5a6799694fc7c7000...|1516738921.1137421|         1.0|Map(questions -> ...|2018-01-23T20:21:...|c0ee680e-8892-4e6...|\n",
      "|6442707e-7488-11e...|        false|Introduction to B...| 1516737000.212122|5a6791e824fccd000...| 1516737000.212122|         1.0|Map(questions -> ...|2018-01-23T19:48:...|e4525b79-7904-405...|\n",
      "|8b4488de-43a5-4ff...|        false|        Learning Git| 1516740790.309757|5a67a0b6852c2a000...| 1516740790.309757|         1.0|Map(questions -> ...|2018-01-23T20:51:...|3186dafa-7acf-47e...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746279.3801291|5a67b627cc80e6000...|1516746279.3801291|         1.0|Map(questions -> ...|2018-01-23T22:24:...|48d88326-36a3-4cb...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743820.305464|5a67ac8cb0a5f4000...| 1516743820.305464|         1.0|Map(questions -> ...|2018-01-23T21:43:...|bb152d6b-cada-41e...|\n",
      "|1a233da8-e6e5-48a...|        false|Intermediate Pyth...|  1516743098.56811|5a67a9ba060087000...|  1516743098.56811|         1.0|Map(questions -> ...|2018-01-23T21:31:...|70073d6f-ced5-4d0...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743764.813107|5a67ac54411aed000...| 1516743764.813107|         1.0|Map(questions -> ...|2018-01-23T21:42:...|9eb6d4d6-fd1f-4f3...|\n",
      "|4cdf9b5f-fdb7-4a4...|        false|A Practical Intro...|1516744091.3127241|5a67ad9b2ff312000...|1516744091.3127241|         1.0|Map(questions -> ...|2018-01-23T21:45:...|093f1337-7090-457...|\n",
      "|e1f07fac-5566-4fd...|        false|Git Fundamentals ...|1516746256.5878439|5a67b610baff90000...|1516746256.5878439|         1.0|Map(questions -> ...|2018-01-23T22:24:...|0f576abb-958a-4c0...|\n",
      "|87b4b3f9-3a86-435...|        false|Introduction to M...|  1516743832.99235|5a67ac9837b82b000...|  1516743832.99235|         1.0|Map(questions -> ...|2018-01-23T21:40:...|0c18f48c-0018-450...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743332.7596769|5a67aaa4f21cc2000...|1516743332.7596769|         1.0|Map(questions -> ...|2018-01-23T21:34:...|b38ac9d8-eef9-495...|\n",
      "|7e2e0b53-a7ba-458...|        false|Introduction to P...| 1516743750.097306|5a67ac46f7bce8000...| 1516743750.097306|         1.0|Map(questions -> ...|2018-01-23T21:41:...|bbc9865f-88ef-42e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744410.4791961|5a67aedaf34e85000...|1516744410.4791961|         1.0|Map(questions -> ...|2018-01-23T21:51:...|8a0266df-02d7-44e...|\n",
      "|e5602ceb-6f0d-11e...|        false|Python Data Struc...|1516744446.3999851|5a67aefef5e149000...|1516744446.3999851|         1.0|Map(questions -> ...|2018-01-23T21:53:...|95d4edb1-533f-445...|\n",
      "|f432e2e3-7e3a-4a7...|        false|Working with Algo...| 1516744255.840405|5a67ae3f0c5f48000...| 1516744255.840405|         1.0|Map(questions -> ...|2018-01-23T21:50:...|f9bc1eff-7e54-42a...|\n",
      "|76a682de-6f0c-11e...|        false|Learning iPython ...| 1516744023.652257|5a67ad579d5057000...| 1516744023.652257|         1.0|Map(questions -> ...|2018-01-23T21:46:...|dc4b35a7-399a-4bd...|\n",
      "|a7a65ec6-77dc-480...|        false|   Python Epiphanies|1516743398.6451161|5a67aae6753fd6000...|1516743398.6451161|         1.0|Map(questions -> ...|2018-01-23T21:35:...|d0f8249a-597e-4e1...|\n",
      "+--------------------+-------------+--------------------+------------------+--------------------+------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_assessments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out to HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extracted_assessments.write.mode(\"overwrite\").parquet(\"/tmp/assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sequences Table \n",
    "\n",
    "The `sequence_id` is nested within the `sequences` dictionary. The lambda function below pulls out the `sequences_id` using the [] operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_sequences_id(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"sequences_id\" : raw_dict[\"sequences\"][\"id\"]}\n",
    "    return Row(**my_dict)\n",
    "\n",
    "my_sequences = assessments.rdd.map(my_lambda_sequences_id).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences.registerTempTable('sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out to hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_sequences.write.mode(\"overwrite\").parquet(\"/tmp/sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|sequences_id                        |\n",
      "+------------------------------------+\n",
      "|ced04db4-0842-47ee-bfa2-8cf45db9f433|\n",
      "|355710e7-c2ad-46bc-8844-0cb71a9148f0|\n",
      "|7214e626-ac4b-42be-ad86-109e764c6d85|\n",
      "|9acbffb6-6ec0-41e5-aaf0-f24bb64040de|\n",
      "|cec4308a-64dc-484e-ac96-f87e74c33a75|\n",
      "|3585eaaa-512d-4ff5-9f36-aeb54477b1c5|\n",
      "|1a64dcab-b2d8-4fd0-a294-62ef4b090491|\n",
      "|1be180d1-2c0b-4ca9-b39c-09bf3d3761a9|\n",
      "|f02fa40c-5572-458f-b1ed-2d45248df61c|\n",
      "|0b14f3a0-7068-411f-bad6-e0eb4cc64e6e|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select distinct sequences_id from sequences limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sql joins together the assessments temporary table and the sequences temporary table, joining over the `keen_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+\n",
      "|             keen_id|    keen_timestamp|        sequences_id|\n",
      "+--------------------+------------------+--------------------+\n",
      "|5a17a67efa1257000...|1511499390.3836269|8ac691f8-8c1a-403...|\n",
      "|5a26ee9cbf5ce1000...|1512500892.4166169|9bd87823-4508-4e0...|\n",
      "|5a29dcac74b662000...|1512692908.8423469|e7110aed-0d08-4cb...|\n",
      "|5a2fdab0eabeda000...|1513085616.2275269|cd800e92-afc3-447...|\n",
      "|5a30105020e9d4000...|1513099344.8624721|8ac691f8-8c1a-403...|\n",
      "|5a3a6fc3f0a100000...| 1513779139.354213|e7110aed-0d08-4cb...|\n",
      "|5a4e17fe08a892000...|1515067390.1336551|9abd5b51-6bd8-11e...|\n",
      "|5a4f3c69cc6444000...| 1515142249.858722|083844c5-772f-48d...|\n",
      "|5a51b21bd0480b000...| 1515303451.773272|e7110aed-0d08-4cb...|\n",
      "|5a575a85329e1a000...| 1515674245.348099|25ca21fe-4dbb-446...|\n",
      "+--------------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id from assessments a join sequences s on a.keen_id = s.keen_id limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Questions Table\n",
    "\n",
    "This table assigns a number to each problem in a user's test. Here we assume that the order of the ids stored in the value of the questions is the same order the questions appear to the exam user. This table also includes boolean values for if the user was correct if the question was incomplete, and if the user submitted this question. \n",
    "\n",
    "The lambda function below extracts information from multi-level nest dictionaries. We do not assume that user_correct, user_imcomplete, and user_submitted exist for each question. As such, we must check these keys exist in the questions dictionary. If they do not exists they are cast as NULL otherwise their value is added to the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lambda_questions(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    my_count = 0\n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        my_count += 1\n",
    "        for x in [\"user_correct\", \"user_incomplete\",\"user_submitted\"]:\n",
    "            if x in l:\n",
    "                continue\n",
    "            else:\n",
    "                l[x]=None\n",
    "        my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"number\" : my_count, \"question_id\" : l[\"id\"], \n",
    "                  \"correct\" : l[\"user_correct\"], \"incomplete\" : l[\"user_incomplete\"], \n",
    "                   \"submitted\" : l[\"user_submitted\"]}\n",
    "        my_list.append(Row(**my_dict))\n",
    "    return my_list\n",
    "\n",
    "my_questions = assessments.rdd.flatMap(my_lambda_questions).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_questions.registerTempTable('questions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out to hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_questions.write.mode(\"overwrite\").parquet(\"/tmp/questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL example queries\n",
    "\n",
    "This query only pulls information from the questions table. The query result shows the question id, the number of the question on the user's exam, the result, and incomplete status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------+----------+\n",
      "|         question_id|number|correct|incomplete|\n",
      "+--------------------+------+-------+----------+\n",
      "|7a2ed6d3-f492-49b...|     1|  false|      true|\n",
      "|bbed4358-999d-446...|     2|  false|     false|\n",
      "|e6ad8644-96b1-461...|     3|   true|     false|\n",
      "|95194331-ac43-454...|     4|   true|     false|\n",
      "|95194331-ac43-454...|     1|   true|     false|\n",
      "|bbed4358-999d-446...|     2|  false|      true|\n",
      "|e6ad8644-96b1-461...|     3|  false|     false|\n",
      "|7a2ed6d3-f492-49b...|     4|  false|      true|\n",
      "|b9ff2e88-cf9d-4bd...|     1|  false|     false|\n",
      "|bec23e7b-4870-49f...|     2|   true|     false|\n",
      "+--------------------+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select question_id, number, correct, incomplete from questions limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below joins the assessments table and questions table to include the exam name in addition to the question id, number of the question within an exam, and the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------------+-------+\n",
      "|             keen_id|         question_id|           exam_name|question_number|correct|\n",
      "+--------------------+--------------------+--------------------+---------------+-------+\n",
      "|5a4f3c69cc6444000...|a6effaf7-94ba-458...|A Practical Intro...|              1|   true|\n",
      "|5a4f3c69cc6444000...|dab47905-63c6-46b...|A Practical Intro...|              2|   true|\n",
      "|5a4f3c69cc6444000...|7ff41d87-1f73-406...|A Practical Intro...|              3|   true|\n",
      "|5a4f3c69cc6444000...|80aad87e-c7b2-4e1...|A Practical Intro...|              4|   true|\n",
      "|5a3c02bc6863ce000...|80aad87e-c7b2-4e1...|A Practical Intro...|              1|  false|\n",
      "|5a3c02bc6863ce000...|a6effaf7-94ba-458...|A Practical Intro...|              2|   true|\n",
      "|5a3c02bc6863ce000...|dab47905-63c6-46b...|A Practical Intro...|              3|  false|\n",
      "|5a3c02bc6863ce000...|7ff41d87-1f73-406...|A Practical Intro...|              4|   true|\n",
      "|5a4f3ba34ce2fc000...|80aad87e-c7b2-4e1...|A Practical Intro...|              1|  false|\n",
      "|5a4f3ba34ce2fc000...|a6effaf7-94ba-458...|A Practical Intro...|              2|  false|\n",
      "+--------------------+--------------------+--------------------+---------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select q.keen_id, q.question_id, a.exam_name, q.number as question_number, q.correct from assessments a join questions q on a.keen_id = q.keen_id  where exam_name = 'A Practical Introduction to React.js' limit 10\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below joins all three previously created tables to combine this information. This new table can be stored as a view or temporary table for additional queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+------+\n",
      "|             keen_id|    keen_timestamp|        sequences_id|         question_id|           exam_name|number|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+------+\n",
      "|5a156f32621ed9000...|1511354162.7768071|ced04db4-0842-47e...|5a244dab-fea1-492...|Networking for Pe...|     1|\n",
      "|5a156f32621ed9000...|1511354162.7768071|ced04db4-0842-47e...|3fd6f8d9-3f2a-4d8...|Networking for Pe...|     2|\n",
      "|5a156f32621ed9000...|1511354162.7768071|ced04db4-0842-47e...|09201e96-9485-44a...|Networking for Pe...|     3|\n",
      "|5a1579f36d8efa000...|  1511356915.58588|ced04db4-0842-47e...|3fd6f8d9-3f2a-4d8...|Networking for Pe...|     1|\n",
      "|5a1579f36d8efa000...|  1511356915.58588|ced04db4-0842-47e...|5a244dab-fea1-492...|Networking for Pe...|     2|\n",
      "|5a1579f36d8efa000...|  1511356915.58588|ced04db4-0842-47e...|09201e96-9485-44a...|Networking for Pe...|     3|\n",
      "|5a1571614c46f3000...|  1511354721.72008|ced04db4-0842-47e...|09201e96-9485-44a...|Networking for Pe...|     1|\n",
      "|5a1571614c46f3000...|  1511354721.72008|ced04db4-0842-47e...|3fd6f8d9-3f2a-4d8...|Networking for Pe...|     2|\n",
      "|5a1571614c46f3000...|  1511354721.72008|ced04db4-0842-47e...|5a244dab-fea1-492...|Networking for Pe...|     3|\n",
      "|5a1573794aad1c000...| 1511355257.564451|ced04db4-0842-47e...|3fd6f8d9-3f2a-4d8...|Networking for Pe...|     1|\n",
      "|5a1573794aad1c000...| 1511355257.564451|ced04db4-0842-47e...|5a244dab-fea1-492...|Networking for Pe...|     2|\n",
      "|5a1573794aad1c000...| 1511355257.564451|ced04db4-0842-47e...|09201e96-9485-44a...|Networking for Pe...|     3|\n",
      "|5a156c116c7571000...|1511353361.6311541|ced04db4-0842-47e...|5a244dab-fea1-492...|Networking for Pe...|     1|\n",
      "|5a156c116c7571000...|1511353361.6311541|ced04db4-0842-47e...|3fd6f8d9-3f2a-4d8...|Networking for Pe...|     2|\n",
      "|5a156c116c7571000...|1511353361.6311541|ced04db4-0842-47e...|09201e96-9485-44a...|Networking for Pe...|     3|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select q.keen_id, a.keen_timestamp, s.sequences_id, q.question_id, a.exam_name, q.number from assessments a join questions q on a.keen_id = q.keen_id join sequences s on s.keen_id = q.keen_id where s.sequences_id = 'ced04db4-0842-47ee-bfa2-8cf45db9f433' limit 30\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Time Table\n",
    "\n",
    "This table pulls the starting time and ending time for each exam. Ending time is the final (maximum) timestamp for each assessment. Ending time is extracted from a dictionary nested inside a list that is nested inside multiple dictionaries. Not all questions have an `at` key which is a possible ending time. \n",
    "\n",
    "The lambda function below finds all `at` times given for each question on an assessment. It then assigns the latest (max) `at` time as the ending time for the assessment. To maximize parallelization, the lambda function converts both the starting time and ending time into an easier-to-use DateTime format and then calculates the exam time in seconds. \n",
    "\n",
    "Note: There are clear anomalies in the time data. The minimum exam time is approximately -88 seconds and there are 18 assessments, roughly 0.5%, that return a negative exam time. As the end-time generated by the lambda function is the maximum of all `at` times this implies that all `at` times in that assessment occur before the end time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def my_lambda_times(x):\n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    times = []\n",
    "    \n",
    "    for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "        if \"options\" in l:\n",
    "            for j in l[\"options\"]:\n",
    "                if \"at\" in j:\n",
    "                    times.append(j['at'])\n",
    "                    \n",
    "    start_time = datetime.strptime(raw_dict[\"started_at\"] , '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    if len(times) == 0:\n",
    "        total = 0\n",
    "        end_time = None\n",
    "    else:\n",
    "        end_time = datetime.strptime(max(times) , '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "        total = (end_time - start_time).total_seconds()\n",
    "        \n",
    "    my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"started_at\" : start_time, \"ended_at\" : end_time,\n",
    "                \"exam_time\" : total }\n",
    "    my_list.append(Row(**my_dict))\n",
    "    return my_list\n",
    "\n",
    "my_times = assessments.rdd.flatMap(my_lambda_times).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+------------------------+-----------------------+\n",
      "|ended_at               |exam_time|keen_id                 |started_at             |\n",
      "+-----------------------+---------+------------------------+-----------------------+\n",
      "|2018-01-23 14:24:00.807|41.725   |5a6745820eb8ab00016be1f1|2018-01-23 14:23:19.082|\n",
      "|2018-01-23 14:22:55.494|67.989   |5a674541ab6b0a0001c6e723|2018-01-23 14:21:47.505|\n",
      "|2018-01-23 20:22:52.423|29.839   |5a67999d3ed3e300016ef0f1|2018-01-23 20:22:22.584|\n",
      "|2018-01-23 20:21:59.075|48.242   |5a6799694fc7c70001034706|2018-01-23 20:21:10.833|\n",
      "|2018-01-23 19:49:56.912|74.435   |5a6791e824fccd00018c3ff9|2018-01-23 19:48:42.477|\n",
      "|2018-01-23 20:53:08.554|81.272   |5a67a0b6852c2a00018891fa|2018-01-23 20:51:47.282|\n",
      "|2018-01-23 22:30:58.374|384.182  |5a67b627cc80e60001343664|2018-01-23 22:24:34.192|\n",
      "|2018-01-23 21:43:39.445|33.296   |5a67ac8cb0a5f400017d9919|2018-01-23 21:43:06.149|\n",
      "|2018-01-23 21:31:31.708|23.248   |5a67a9ba0600870001247a04|2018-01-23 21:31:08.46 |\n",
      "|2018-01-23 21:42:43.106|5.6      |5a67ac54411aed0001da9129|2018-01-23 21:42:37.506|\n",
      "|2018-01-23 21:48:10.191|163.834  |5a67ad9b2ff31200017b6c58|2018-01-23 21:45:26.357|\n",
      "|2018-01-23 22:30:35.546|394.649  |5a67b610baff9000015bc9f6|2018-01-23 22:24:00.897|\n",
      "|2018-01-23 21:43:51.049|211.622  |5a67ac9837b82b0001ea6011|2018-01-23 21:40:19.427|\n",
      "|2018-01-23 21:35:30.998|53.771   |5a67aaa4f21cc20001e373c8|2018-01-23 21:34:37.227|\n",
      "|2018-01-23 21:42:29.098|50.558   |5a67ac46f7bce80001ad43e3|2018-01-23 21:41:38.54 |\n",
      "|2018-01-23 21:53:29.275|120.269  |5a67aedaf34e850001061eeb|2018-01-23 21:51:29.006|\n",
      "|2018-01-23 21:54:05.449|11.133   |5a67aefef5e1490001222a7e|2018-01-23 21:53:54.316|\n",
      "|2018-01-23 21:50:54.398|24.675   |5a67ae3f0c5f48000158e3d2|2018-01-23 21:50:29.723|\n",
      "|2018-01-23 21:47:02.402|8.573    |5a67ad579d50570001d5e379|2018-01-23 21:46:53.829|\n",
      "|2018-01-23 21:36:37.335|41.793   |5a67aae6753fd60001df7224|2018-01-23 21:35:55.542|\n",
      "+-----------------------+---------+------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_times.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_times.registerTempTable('times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out to hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_times.write.mode(\"overwrite\").parquet(\"/tmp/times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL example queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary data for all assessments in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----------------+-----------------+\n",
      "|min_min |max_min          |avg_min          |std_min          |\n",
      "+--------+-----------------+-----------------+-----------------+\n",
      "|-1.46735|963.7744166666666|7.360868511722728|60.54505247064996|\n",
      "+--------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select min(exam_time)/60 as min_min, max(exam_time)/60 as max_min, avg(exam_time)/60 as avg_min, std(exam_time)/60 as std_min from times t limit 20\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary data broken down by exam. This is down by joining the times temporary table and assessments temporary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-------+------------------+---------+\n",
      "|exam_name                                           |min_sec|avg_sec           |max_sec  |\n",
      "+----------------------------------------------------+-------+------------------+---------+\n",
      "|HTML5 The Basics                                    |11.212 |11210.113038461535|57826.465|\n",
      "|Architectural Considerations for Hadoop Applications|24.135 |2549.5588         |24861.206|\n",
      "|Client-Side Data Storage for Web Developers         |131.608|2362.5325000000003|4593.457 |\n",
      "|Introduction to Shiny                               |7.686  |2038.1042962962965|23624.016|\n",
      "|Getting Ready for Angular 2                         |22.917 |1133.0826666666667|3325.964 |\n",
      "|Hadoop Fundamentals for Data Scientists             |11.708 |1079.3948333333333|6356.736 |\n",
      "|Practical Java Programming                          |9.521  |967.199811320755  |37912.611|\n",
      "|Using Web Components                                |34.801 |888.8933333333333 |2568.447 |\n",
      "|Relational Theory for Computer Professionals        |22.464 |831.0181333333334 |5686.44  |\n",
      "|Introduction to Java 8                              |-18.991|724.6883860759496 |41450.925|\n",
      "+----------------------------------------------------+-------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.exam_name, min(t.exam_time) as min_sec, avg(t.exam_time) as avg_sec, max(t.exam_time) as max_sec from times t join assessments a on t.keen_id = a.keen_id group by exam_name order by avg_sec desc limit 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessments in the database with negative exam times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+---------+------------------------+-----------------------+\n",
      "|ended_at               |exam_time|keen_id                 |started_at             |\n",
      "+-----------------------+---------+------------------------+-----------------------+\n",
      "|2017-12-25 17:11:08.649|-88.041  |5a41318a1786c600014c7978|2017-12-25 17:12:36.69 |\n",
      "|2017-12-25 17:09:51.873|-29.84   |5a41313d5682ed000101d065|2017-12-25 17:10:21.713|\n",
      "|2017-12-25 17:10:47.386|-63.75   |5a413176f21cc20001f47b56|2017-12-25 17:11:51.136|\n",
      "|2018-01-07 13:48:54.597|-1.996   |5a52254b4fc7c7000149956b|2018-01-07 13:48:56.593|\n",
      "|2017-12-11 14:30:31.919|-60.501  |5a2e96e4aaa9b70001d283b6|2017-12-11 14:31:32.42 |\n",
      "|2018-01-25 10:20:17.121|-24.823  |5a69af8b0a940000016a73f8|2018-01-25 10:20:41.944|\n",
      "|2018-01-17 05:00:04.452|-18.991  |5a5ed87fbe96950001e665e2|2018-01-17 05:00:23.443|\n",
      "|2018-01-01 22:03:41.578|-9.148   |5a4ab04b08a1890001f55fed|2018-01-01 22:03:50.726|\n",
      "|2018-01-01 22:03:33.82 |-9.178   |5a4ab043d93f790001c1ac36|2018-01-01 22:03:42.998|\n",
      "|2018-01-01 22:03:25.538|-7.383   |5a4ab03bb61b2600012441ca|2018-01-01 22:03:32.921|\n",
      "|2018-01-14 16:20:01.363|-8.829   |5a5b833d0d6f2300014d4c36|2018-01-14 16:20:10.192|\n",
      "|2018-01-14 16:19:49.321|-8.796   |5a5b8330753d900001fbff4b|2018-01-14 16:19:58.117|\n",
      "|2018-01-14 16:20:08.523|-6.8     |5a5b834346f028000110e53b|2018-01-14 16:20:15.323|\n",
      "|2018-01-14 16:19:44.006|-7.977   |5a5b832b88cd280001085f19|2018-01-14 16:19:51.983|\n",
      "|2018-01-11 12:48:12.189|-15.551  |5a575d2fc07ea800014ad8b8|2018-01-11 12:48:27.74 |\n",
      "|2017-12-31 03:43:14.445|-43.058  |5a485d2f765fa10001cd9f69|2017-12-31 03:43:57.503|\n",
      "|2017-12-19 16:28:20.911|-14.777  |5a393e3808fdab00012199d3|2017-12-19 16:28:35.688|\n",
      "|2017-12-25 17:10:47.386|-63.75   |5a413176f21cc20001f47b56|2017-12-25 17:11:51.136|\n",
      "+-----------------------+---------+------------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from times t where(exam_time<0)  limit 30\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Results Table \n",
    "\n",
    "The custom lambda function below unrolls nested exam data relevant to the responses for each exam attempt in the data. This nesting is three levels deep. A new temporary table `results` contains the number of `keen_id`, the number of correct responses, and the number of total questions on the exam. We assume here that all assessments contain values for `correct` and `total`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_lambda_results(x):\n",
    "    \n",
    "    raw_dict = json.loads(x.value)\n",
    "    my_list = []\n",
    "    \n",
    "    if \"sequences\" in raw_dict:\n",
    "        \n",
    "        if \"counts\" in raw_dict[\"sequences\"]:\n",
    "            \n",
    "            if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "                    \n",
    "                my_dict = {\"keen_id\" : raw_dict[\"keen_id\"],\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"], \n",
    "                           \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "                my_list.append(Row(**my_dict))\n",
    "    \n",
    "    return my_list\n",
    "\n",
    "my_results = assessments.rdd.flatMap(my_lambda_results).toDF()\n",
    "\n",
    "my_results.registerTempTable('results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out to hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_results.write.mode(\"overwrite\").parquet(\"/tmp/results\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL example queries\n",
    "\n",
    "Using sql queries we can join this table with the previous assessments table, to get the exam name, and then calculate the exam score using `correct/total*100`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+-----+-----+\n",
      "|             keen_id|           exam_name|correct|total|score|\n",
      "+--------------------+--------------------+-------+-----+-----+\n",
      "|5a17a67efa1257000...|Intermediate Pyth...|      1|    4| 25.0|\n",
      "|5a26ee9cbf5ce1000...|Learning to Progr...|      7|    7|100.0|\n",
      "|5a29dcac74b662000...|        Learning Git|      4|    5| 80.0|\n",
      "|5a2fdab0eabeda000...|        Learning SQL|      3|    4| 75.0|\n",
      "|5a30105020e9d4000...|Intermediate Pyth...|      1|    4| 25.0|\n",
      "|5a3a6fc3f0a100000...|        Learning Git|      5|    5|100.0|\n",
      "|5a4e17fe08a892000...|Introduction to H...|      0|    1|  0.0|\n",
      "|5a4f3c69cc6444000...|A Practical Intro...|      4|    4|100.0|\n",
      "|5a51b21bd0480b000...|        Learning Git|      5|    5|100.0|\n",
      "|5a575a85329e1a000...|Beginning C# Prog...|      3|    4| 75.0|\n",
      "+--------------------+--------------------+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.keen_id, a.exam_name, r.correct, r.total, r.correct / r.total*100 as score from assessments a join results r on a.keen_id = r.keen_id limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average score across all exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|average_score|\n",
      "+-------------+\n",
      "|        62.66|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select round(avg(correct / total)*100,2) as average_score from results limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation of scores across all exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|standard_deviation|\n",
      "+------------------+\n",
      "|31.086692286170553|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select stddev(correct / total)*100 as standard_deviation from results limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sample Buisness Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find several examples of business questions that can be answered using this dataset and the temporary tables created using the custom lambda functions above. Each example below provides the business question, corresponding SQL query, and output (limited to 10 lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many assessments are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|3280 |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as count from assessments\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many people took [insert exam]?\n",
    "This example is for the course \"Introduction to Python\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|162  |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) as count from assessments where exam_name = 'Introduction to Python'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What are the top 10 most popular assessments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------+-----+\n",
      "|exam_name                                                  |count|\n",
      "+-----------------------------------------------------------+-----+\n",
      "|Learning Git                                               |394  |\n",
      "|Introduction to Python                                     |162  |\n",
      "|Introduction to Java 8                                     |158  |\n",
      "|Intermediate Python Programming                            |158  |\n",
      "|Learning to Program with R                                 |128  |\n",
      "|Introduction to Machine Learning                           |119  |\n",
      "|Software Architecture Fundamentals Understanding the Basics|109  |\n",
      "|Beginning C# Programming                                   |95   |\n",
      "|Learning Eclipse                                           |85   |\n",
      "|Learning Apache Maven                                      |80   |\n",
      "+-----------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(*) as count from assessments group by exam_name order by count desc limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which assessments have the most retakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------+-------+\n",
      "|exam_name                                              |retakes|\n",
      "+-------------------------------------------------------+-------+\n",
      "|Beginning C# Programming                               |12     |\n",
      "|Learning C# Best Practices                             |6      |\n",
      "|Learning Git                                           |4      |\n",
      "|Learning C# Design Patterns                            |4      |\n",
      "|Intermediate C# Programming                            |4      |\n",
      "|Introduction to Big Data                               |2      |\n",
      "|An Introduction to d3.js: From Scattered to Scatterplot|2      |\n",
      "|Learning DNS                                           |2      |\n",
      "|Intermediate Python Programming                        |2      |\n",
      "|A Practical Introduction to React.js                   |0      |\n",
      "+-------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select exam_name, count(*)-count(distinct user_exam_id) as retakes from assessments group by exam_name order by retakes desc limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the five assessments with the lowest average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+------------------+\n",
      "|exam_name                                  |avg_score         |\n",
      "+-------------------------------------------+------------------+\n",
      "|Client-Side Data Storage for Web Developers|20.0              |\n",
      "|Native Web Apps for Android                |25.0              |\n",
      "|View Updating                              |25.0              |\n",
      "|Arduino Prototyping Techniques             |33.33333333333333 |\n",
      "|Mastering Advanced Git                     |36.029411764705884|\n",
      "+-------------------------------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.exam_name, avg(s.correct / s.total)*100 as avg_score from assessments a join ct s on a.keen_id = s.keen_id group by a.exam_name order by avg_score asc limit 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the five assessments with the highest average score?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-----------------+\n",
      "|exam_name                                        |avg_score        |\n",
      "+-------------------------------------------------+-----------------+\n",
      "|The Closed World Assumption                      |100.0            |\n",
      "|Learning to Visualize Data with D3.js            |100.0            |\n",
      "|Nulls, Three-valued Logic and Missing Information|100.0            |\n",
      "|Learning SQL for Oracle                          |97.72727272727273|\n",
      "|Introduction to Java 8                           |87.59493670886073|\n",
      "+-------------------------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select a.exam_name, avg(s.correct / s.total)*100 as avg_score from assessments a join ct s on a.keen_id = s.keen_id group by a.exam_name order by avg_score desc limit 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What was the most missed question on the Learning Git Exam?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+-----------------+---------------+\n",
      "|question_id                         |percent_incorrect|percent_correct|\n",
      "+------------------------------------+-----------------+---------------+\n",
      "|5cc84ec3-713c-45f0-a9b8-711a369d0dfc|44.33            |55.67          |\n",
      "+------------------------------------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select q.question_id, round(count( case when q.correct='false' then 1 else null end)/count(*)*100,2) as percent_incorrect, round(count( case when q.correct='true' then 1 else null end)/count(*)*100, 2) as percent_correct from questions q join assessments a on q.keen_id = a.keen_id where a.exam_name = 'Learning Git' group by q.question_id order by percent_incorrect desc limit 1\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is there a relationship between when a user is asked a question and their performance on the question?\n",
    "\n",
    "The sequel below returns a table with the total number of incorrect response for each question for each observed numeric placement of that question on the exam for 'Intermediate Python Programming'. This can be used to answer the question above with the additional of some additional statistical exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------+---------+\n",
      "|question_id                         |number|incorrect|\n",
      "+------------------------------------+------+---------+\n",
      "|32fe7d8d-6d89-4db4-a17a-a368c5ea3ca0|3     |28       |\n",
      "|32fe7d8d-6d89-4db4-a17a-a368c5ea3ca0|4     |24       |\n",
      "|32fe7d8d-6d89-4db4-a17a-a368c5ea3ca0|1     |19       |\n",
      "|32fe7d8d-6d89-4db4-a17a-a368c5ea3ca0|2     |19       |\n",
      "|3477bf63-4db1-4351-8d8c-914b5fc46266|2     |1        |\n",
      "|3477bf63-4db1-4351-8d8c-914b5fc46266|1     |1        |\n",
      "|3477bf63-4db1-4351-8d8c-914b5fc46266|3     |1        |\n",
      "|3f9ff9ee-4497-49b2-8c7c-ac1ce5d7bd92|4     |1        |\n",
      "|3f9ff9ee-4497-49b2-8c7c-ac1ce5d7bd92|3     |1        |\n",
      "|5c34cf19-8cfd-4f56-91c2-0a109dc990b9|2     |22       |\n",
      "+------------------------------------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select q.question_id, q.number, count(q.question_id) as incorrect from questions q join assessments a on q.keen_id = a.keen_id where q.correct = 'false' and a.exam_name = 'Intermediate Python Programming' group by q.question_id, q.number order by question_id, incorrect desc limit 10\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shutdown \n",
    "\n",
    "\n",
    "### Clean exit if using Jupyter notebook or pyspark \n",
    "***\n",
    "Clean exit notebook\n",
    "```\n",
    "Save and Checkpoint\n",
    "Close and Halt\n",
    "```\n",
    "\n",
    "Clean exit pyspark\n",
    "```\n",
    "exit()\n",
    "```\n",
    "\n",
    "***\n",
    "\n",
    "### Tear Down Cluster\n",
    "***\n",
    "```\n",
    "docker-compose down\n",
    "```\n",
    "***\n",
    "\n",
    "### Check everything is down\n",
    "***\n",
    "```\n",
    "docker-compose ps\n",
    "docker ps -a\n",
    "```\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
